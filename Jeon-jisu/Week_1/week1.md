# 1장. 한눈에 보는 머신러닝

지도학습

- 분류
- 회귀

비지도학습

- 계층 군집
- 시각화 알고리즘
- 차원축소
    - 특성 추출
- 이상치탐지
- 특이치탐지(매우 깨끗한 훈련 세트가 필요)
- 연관 규칙 학습

준지도학습

- 구글 포토 서비스 (사진으로 학습한 비슷한 얼굴을 찾아줘서 레이블을 추가하면 모든 사람의 이름을 알게됨.)

자기지도학습

- 이미지의 일부를 마스킹하고 마스킹된 부분을 복원할 수 있도록 훈련.(인풋은 마스킹된 이미지, 정답 라벨은 마스킹안된 원본 이미지)

강화학습

- 환경을 관찰해 행동을 실행하고 보상을 많이 받고 벌점을 덜 받기위한 최상의 전략을 학습

## 배치학습

- 가용한 데이터를 모두 사용해 훈련.
- 모델 부패와 데이터 드리프트 야기

## 온라인학습

- 미니배치로 훈련(작은 묶음 단위로 주입하여 훈련)
- 데이터가 도착하는 대로 즉시 학습 가능하여 환경이 급격히 변할때 사용하면 좋음.

사례 기반 학습과 모델 기반 학습

### 홀드아웃검증

- validation까지 다 학습시키려는 것

### 교차검증

- 각 validation set의 평균으로 metric 측정

# 2장. 머신러닝 프로젝트 처음부터 끝까지

노트북vscolab

describe, info 

계층적샘플링

- bins

표준상관계수로 데이터 사이의 연관성을 알 수 있음

원핫인코딩

- 희소행렬

표준화와 정규화

- 데이터스누핑 편향
- 테스트세트를 들여다보면 테스트 세트에서 겉으로 드러난 어떤 패턴에 속아 특정 모델을 선택할 수도 있다. 이 테스트 세트로 일반화 오차를 추정하면 매우 낙관적인 추정이 되어 시스템을 론칭했을때 기대한 성능이 나오지 않을 수 있음.

### 멱법칙분포

- 멱법칙 분포는 특성을 로그값으로 바꾸는 것이 좋다. (가우스분포 = 종모양분포)
- 버킷타이징하는 것이 좋다.
    - 분포를 거의 동일한 크기의 버킷으로 자르고 income_cat특성을 만들었던 것과 비슷함.

멀티모달 분포일때도 버킷타이징이 도움이 된다. 버킷 ID를 수치가 아닌 카테고리화해야한다. 

- 중간 주택 연도와 특정 모드 사이의 유사도를 나타내는 특성을 추가하는 것이다. 유사도 측정에는 RBF를 사용한다.
- RBF : 방사기저함수

# 3장. 분류

- confusion matrix
- ROC Curve (TPR, TNR) vs PRCurve
- OvR전략(OvA전략)
    - One-vs-Rest
    - 구현의 간결함과 확장성
    - 학습 시간이 상대적으로 빨라진다.
    - 일부 클래스가 다른 클래스에 비해 훨씬 많은 샘플을 가지고 있는 경우, 분류기가 불균형한 데이터 세트에 편향될 수 있다
- OvO전략
    - 각 분류기의 훈련에 전체 훈련 세트중 구별할 두 클래스에 해당하는 샘플만 있으면 된다.
    - 각각의 클래스 쌍에 대해 별도의 분류기를 훈련시키는 것
    - 클래스가 많아질수록 필요한 분류기의 수가 기하급수적으로 증가한다는 점입니다
- 클래스에 속한 데이터 양에따라 지지도 가중치를 주기도 한다.

# 4장. 모델 훈련

모델을 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다. 

- 무어-펜로즈 유사역행렬
    - 무어-펜로즈 유사역행렬은 선형 방정식 시스템, 특히 해가 없거나 무수히 많은 경우에 최소 제곱 해를 찾는 데 사용됩니다.

특잇값 분해(SVD)

- 배치 경사 하강법 vs 확률적 경사 하강법 vs 미니배치 경사 하강법
    - 배치 경사 하강법
        - 매 반복(iteration)마다 전체 훈련 데이터 세트를 사용하여 그래디언트(경사)를 계산
        - 상대적으로 안정적인 그래디언트 추정치를 제공하여 더 안정적인 수렴으로 이어질 수 있지만, 때로는 지역 최소값(local minima)에 갇힐 위험도 있다.
        - 적절한 학습률을 찾기위해 반복 횟수를 제한한 그리드 서치 사용
            - 반복횟수를 크게 지정하고 그레이디언트 벡터가 작아지면, 즉 벡터의 노름이 어떤 값보다 작아지면 알고리즘을 중지
    - 확률적 경사 하강법
        - 전체 훈련 세트를 사용하면 매우 느려지게 되어 매 스탭에서 하나의 샘플을 랜덤으로 선택하여 그레이디언트 계산하는 방식. 작은 메모리로도 가능하다.
        - SGD의 장점은 각 단계에서 전체 데이터 세트를 사용하지 않고 일부 샘플만 사용하기 때문에 계산 효율성이 높다는 것이고 특히 대규모 데이터 세트를 다룰 때 유용하다.
    - 미니배치 경사 하강법
        - 미니배치 경사 하강법에서는 한 번에 하나의 샘플이 아닌 소규모의 배치(32개, 64개 샘플)를 무작위로 선택합니다. 이 방법은 개별 샘플이 연속적으로 선택되는 것을 더욱 방지한다.
        - 미니배치를 어느정도 크게 하면 SGD보다 조금 더 규칙적으로 움직이고 SGD보다 손실함수의 최솟값에 더 가까이 도달하게 될 것이다. but, 지역최솟값에서 빠져나오기는 더 힘들다.
        - SGD에 비해 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 성능을 향상시킬 수 있다.
    
    배치 경사하강법의 경우 매 스탭에서 많은 시간이 소요되며 확률적 경사 하강법과 미니배치 경사하강법의 경우도 적절한 학습 스케쥴을 사용하면 최솟값에 도달하는 것을 잊지 않아야한다. 
    
    고차 다항회귀를 사용한다고 좋은 것이 아님. 과대적합이 될 수 있기때문. 
    
    따라서 데이터에 따라 특정차수의 다항회귀가 필요할 수 있다. 
    
    - 모델이 과대적합이 되었는지, 과소적합되었는지 교차검증으로 확인할 수 있음.
        - 훈련데이터 성능이 좋지만 교차검증점수가 나쁘다 → 과대적합
        - 둘 다 좋지 않다 → 과소적합
    - 모델이 과대적합이 되었는지, 과소적합되었는지 학습곡선을 확인할 수 있음.
        - 훈련하는 동안 훈련 세트와 검증세트에서 일정한 간격으로 모델을 평가하고 그 결과를 그래프로 그리면 된다.
        - 위의것을 그냥 그려서 확인한다고 보면 되는데 이게 많이 쓰임.(시각화의 일종)
    
    ### 편향과 분산의 트레이드오프
    
    - 모델의 복잡도가 커지면 분산이 늘어나고 편향은 줄어들지만, 모델의 복잡도가 줄어들면 편향이 커지고 분산이 작아진다.

# 규제가 있는 선형모델

## 1. 릿지 회귀(티호노프 규제)

릿지 회귀는 비용함수에 추가되는 것으로 테스트 세트에서 성능평가를 할때나 예측을 할때는 포함되지 않는다. 

회귀분석에서 모델의 가중치를 작게 유지해 과대적합을 줄이기위해 도입되었다. (모델의 가중치를 작게 유지할수록 랜덤 노이즈보다는 실제 데이터의 기본 구조에 더 집중하게 됩)규제항을 MSE에 도입하는 것이다. 

가중치의 크기만큼을 전체 비용에 더하기때문에, 모델은 전체 비용을 줄이기 위해 가중치 값을 가능한 작게 유지하려고 노력하게 되는 원리. (이때 $\theta$는 가중치 벡터 $\theta_{1}$에서 $\theta_{n}$)

$$
J(\theta)=MSE(\theta)+\frac{
\alpha}{m}\sum_{i=1}^n
\theta_{i}^2

$$

## 2.라쏘 회귀

릿지회귀에서는 규제항으로 L2노름을 사용했지만 라쏘회귀에서는 L1노름을 사용

$$
J(\theta)=MSE(\theta)+2\alpha\sum_{i=1}^{n}∣\theta_{i}∣
$$

라쏘회귀는 비용 함수는 선형 회귀 모델의 예측과 실제 값 사이의 오차를 최소화하면서도, 모델의 가중치의 절대값의 합을 통제합니다. 이러한 특성때문에 모델이 일부 가중치를 정확히 0으로 만들어, 특성 선택의 형태로 작용하게 합니다.

라쏘 비용함수는 $\theta_{i} = 0$일때 미분가능하지않다. 하지만 서브그레이디언트 벡터를 사용하면 경사하강법을 적용할 수 있다. 

*서브그레이디언트 벡터 : 미분 불가능한 지점에서 함수의 기울기를 대략적으로 나타내는 벡터

## 3.엘라스틱넷 회귀

라쏘회귀와 릿지 회귀를 절충. 혼합비율 r을 사용해 혼합정도를 조절한다. 

릿지가 기본이 되지만 몇가지 특성만 유용하다고 생각되면 라쏘나 엘라스틱넷이 낫다. 특성수가 훈련 샘플 수보다 많거나 특성 몇개가 강하게 연관되어있을때는 보통 라쏘가 문제를 일으키므로 엘라스틱넷이 좋다. 

## 로지스틱 회귀

- 샘플이 특정 클래스에 속할 확률을 추정하는데 널리 사용. 추정 확률이 주어진 임곗값보다 크면 모델이 그 샘플이 해당 클래스에 속한다고 예측