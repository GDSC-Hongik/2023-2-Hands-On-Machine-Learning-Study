# 2주차 Summary

생성일: December 17, 2023 1:15 AM
주차: 2주차
깃허브 커밋: No
사람: 최윤창
완료 여부: 진행 중

<aside>
📌 키워드 요약

</aside>

# 10장 케라스를 사용한 인공 신경망 소개

## 인공 뉴런

### **퍼셉트론**

- 가장 간단한 인공 신경망 구조
- TLU라는 인공 뉴런 기반이며 입출력이 이진값이 아닌 숫자이고, 각각의 입력 연결은 가중치와 연관되어 있다.
- TLU는 입력의 가중치 합을 계산한 뒤 계산된 합에 계단 함수를 적용하여 결과를 출력

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled.png)

- **헤비사이드 계단 함수 / 부호 함수 (주로 사용되는 계단 함수)**
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%201.png)
    
- TLU는 간단한 선형 이진 분류에 사용 가능
- 퍼셉트론은 하나의 층 안에 놓인 하나 이상의 TLU로 구성. 각각의 TLU는 모든 입력에 연결 → fully connected layer라고 불림
- input layer는 모두 입력 뉴런으로 구성
- 완전 연결 층의 출력 계산 : hW,b(X)=ϕ(XW+b)
    - X : 입력 특성의 행렬을 나타냄
        - 이 행렬의 행은 샘플, 열은 특성
    - W : 가중치 행렬. 편향 뉴런을 제외한 모든 연결 가중치를 포함
        - 이 행렬의 행은 입력 뉴런에 해당하고 열은 출력층에 있는 인공 뉴런에 해당
    - b : 편향 벡터. 편향 뉴런과 인공 뉴런 사이의 모든 연결 가중치를 포함
        - 인공 뉴런마다 하나의 편향 값이 있다.
    - ϕ : 활성화 함수 (activation function)
- **헤브의 규칙**
    
    두 뉴런이 동일한 출력을 낼 때마다 둘 사이의 연결 가중치가 증가
    
    퍼셉트론 학습 규칙은 오차가 감소되도록 연결을 강화시킨다
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%202.png)
    
- **퍼셉트론 수렴 이론**
    
    훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것을 증명
    
- **다층 퍼셉트론**
    
    퍼셉트론을 여러개 쌓아 일부 제약을 줄일 수 있다.
    
    xor 문제를 해결 가능
    

### 다층 퍼셉트론과 역전파

- 다층 퍼셉트론은 입력층 하나와 은닉층이라 불리는 하나 이상의 TLU층과 마지막 출력 층으로 구성
- 은닉층을 여러개 쌓아 올림 인공 신경망을 **심층 신경망**이라고 한다.
- **역전파**?
    - 후진 자동 미분 + 경사 하강법
    - 작동 방식
        1. 역전파 알고리즘이 먼저 미니배치에 대한 예측을 만들고 오차를 측정(정방향계산)
        2. 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정(역방향계산)
            
            → 연쇄 법칙 적용
            
        3. 오차가 감소하도록 가중치와 편향을 조정(경사 하강법)
    - 계단 함수를 시그모이드 함수로 변경(사실 다른 활성화 함수와도 사용 가능-ReLU, tanh 함수)
    - **활성 함수는 왜?**
        - 선형 층 사이에 비선형함수를 포함해야 하나로 합쳐지지 않고 각 각의 층으로 나누어질 수 있음
        
        ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%203.png)
        

### 회귀를 위한 다층 퍼셉트론

회귀  작업에 사용될 수 있음

- 회귀 MLP의 전형적인 구조
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%204.png)
    

### 분류를 위한 다층 퍼셉트론

분류 작업에 사용될 수 있음

- 시그모이드 활성화 함수를 가진 하나의 출력 뉴런만 필요(출력은 0과 1사이)
- 다층 퍼셉트론은 다중 레이블 이진 분류 문제를 쉽게 처리
- 확률 분포를 예측해야 하므로 손실 함수에는 일반적으로 크로스 엔트로피 손실을 선택하는 것이 좋다
- 분류  MLP의 전형적인 구조
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%205.png)
    

## 케라스로 다층 퍼셉트론 구현하기

~

## 신경망 하이퍼파라미터 튜닝하기

조절할 하이퍼파라미터가 많다 → 어떤 조합이 주어진 문제에 최적일까?

케라스 모델을 위한 하이퍼파라미터 튜닝 라이브러리인 케라스 튜너 라이브러리 사용

### 은닉 층 개수 / 은닉 층의 뉴런 개수

- 은닉 층이 하나인 다층 퍼셉트론이더라도 뉴런 개수가 충분하면 아주 복잡한 함수도 모델링할 수 있음
- 하지만 복잡한 문제에서는 심층 신경망이 파라미터 효율성이 좋다 → 적은 수 의 뉴런을 사용하므로 동일한 양의 훈련 데이터에서 더 높은 성능을 낼 수 있다.
- 층의 개수와 뉴런의 개수는 과대적합이 시작되기 전까지 점진적으로 뉴런 수를 늘릴 수 있다. → 일반적으로는 층의 수를 늘리는 쪽이 이득이 많다.

### 학습률, 배치 크기, 하이퍼파라미터

- **학습률**
    - 일반적으로 최적의 학습률은 최대 학습률의 절반
    - 매우 낮은 학습률에서 시작해서 점진적으로 매우 큰 학습률까지 수백 번 반복하여 모델을 훈련 → 반복마다 일정한 값을 학습률에 곱한다
    - 최적의 학습률은 손실이 상승하는 지점보다 조금 아래에 있을것
- **옵티마이저**
    - 미니배치 경사하강법보다 더 좋은 옵티마이저를 선택하는 것도 매우 중요
- **배치 크기**
    - 큰 배치 크기를 사용
        - GPU같은 하드웨어 가속기를 효율적으로 활용
        - 훈련 초기 종종 불안정하게 훈련될 수 있음 → 학습률 예열을 사용해 큰 배치 크기를 시도/ 훈련이 불안정하다면 작은 배치 크기를 사용해본다
- **활성화 함수**
    - 일반적으로 ReLU가 좋은 기본값
- **반복 횟수**
    - 조기 종료를 사용

# 11장 심층 신경망 훈련

## gradient 소실과 폭주

### 글로럿과 He 초기화

### 고급 활성화 함수

- ReLU에 문제점이 있다
    
    → 훈련하는 동안 일부 뉴런이 0 이외의 값을 출력하지 않는다
    
- LeakyReLU
    - LeakyReLUα(z)=max(αz,z)
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%206.png)
    
    기울기를 0이 아닌 다른 작은 수로 설정하여 0이 되지 않게 함.
    
- ELU
    
    훈련 시간이 줄고 신경망의 테스트 세트 성능도 더 높았다
    
    다만 지수함수를 사용하므로 계산이 느리다
    

### 배치 정규화

- 그레디언트 소실과 폭주 문제를 해결하기 위한 배치 정규화 기법을 제안
- 각 층에서 활성화 함수를 통과하기 전이 나 후에 모델에 연산을 추가
    - 입력을 원점에 맞추고 정규화 한 다음, 각 층에서 2개의 새로운 피라미터로 결괏값의 스케일을 조정하고 이동
    - 평균은 0으로 분산은 스케일 조정한다
- 훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 계산하는 것이다.
- 대부분 배치 정규화 구현은 층의 입력 평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계를 추정한다.
- 배치 정규화 층마다 네 개의 파라미터 벡터가 학습된다.
    - **γ**(출력 스케일 벡터)와 **β** ( 출력 이동 벡터)는 일반적인 역전파를 통해 학습된다.
    - **μ** (최종 입력 평균 벡터)와 **σ** (최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정된다.
    - **μ**와 **σ**는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용된다
- 장점
    - 그레디언트 소실 문제가 크게 감소
    - 가중치 초기화 덜 민감해진다
    - 규제와 같은 역할을 하여 다른 규제의 필요성을 줄여준다
- 단점
    - 모델의 복잡도를 키운다
    - 실행 시간면에서 손해(층마다 추가되는 계산 시간)

### gradient clipping

- 역전파될 때 특정 임곗값을 넘어서지 못하게 그레디언트를 잘라낸다
    - 배치 정규화를 사용하기 힘든 순환 신경망에서 사용

## 사전 훈련된 층 재사용하기

- 비슷한 유형의 문제를 처리한 신경망이 있다면 최상위 층을 제외하고 대부분의 층을 재사용할 수 있다.(전이학습)
- 보통 원본 모델의 출력 층을 바꾼다. 비슷한 원본 모델의 하위 은닉층이 훨씬 유용하다.

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%207.png)

- 방법
    1. 재사용하는 층을 모두 동결하고 훈련 및 성능 평가 (즉, 경사 하강법으로 가중치가 고정된 상태로 남도록 훈련되지 않는 가중치로 만든다)
    2. 맨 위에 있는 한두개의 은닉 층의 동결을 해체하고 역전파를 통해 가중치 조정하여 성능 향상 확인
        
        → 훈련 데이터가 많을수록 많은 층의 동결 해제 가능
        
        → 재사용 층의 동결을 해제할 때는 학습률을 줄이는것이 좋음
        
    3. 좋은 성능이 안나오거나 훈련 데이터가 적다면 상위 은닉 층들을 제거하고 남은 은닉 층을 다시 동결
        
        → 적절한 개수의 은닉 층을 찾을 때까지 반복
        
        → 훈련 데이터가 아주 많다면 은닉 층 제거 대신 다른 것으로 바꾸거나 은닉층 추가도 가능
        

### 비지도 사전 훈련

- 풀어야 할 문제가 복잡하고 재사용할 수 있는 비슷한 모델이 없으며 레이블된 훈련 데이터가 적을때 비지도 사전 훈련이 좋다.
- 레이블되지 않은 훈련 데이터를 많이 모을 수 있다면 이를 사용하여 오토인코더나 생성적 적대 신경망과 같은 비지도 학습 모델 훈련 가능
    - 오토인코더나 GAN판별자의 하위층을 재사용하고 그 위에 새로운 착업에 맞는 출력층 추가
    - 그 다음 지도학습으로 최종 네트워크를 세밀하게 튜닝
    

### 보조 작업에서 사전 훈련

- 레이블된 훈련 데이터가 많지 않다면 레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 훈련한다
    
    → 이 신경망의 하위층을 실제 작업을 위해 재사용
    

## 고속 옵티마이저

### 모멘텀 최적화

### ****네스테로프 가속 경사****

- AdaGrad
- RMSProp
- Adam, Nadam

### 학습률 스케줄링

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%204f9eb77affe448bb9a78151e584bbeb9/Untitled%208.png)

- 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮춰 좋은 솔루션을 더 빨리 발견
- 거듭제곱 기반 스케줄링
- 지수기반 스케줄링
- 구간별 고정 스케줄링
- 성능 기반 스케줄링
- 1사이클 스케줄링

## 규제를 사용해 과대적합 피하기

### l1과 l2규제

### 드롭아웃

- 각 뉴런은 p의 확률로 드롭아웃 되게 된다.(p는 10-50%사이)
- 몬테 카를로 드롭아웃

### 맥스-노름 규제