# 2주차 Summary

생성일: 2023년 12월 17일 오전 1:14
주차: 2주차
깃허브 커밋: No
사람: 윤정 이
완료 여부: 진행 중

<aside>
📌 키워드 요약

</aside>

## **10장.** 케라스를 사용한 인공 신경망 소개

---

### 예전과 다르게 인공 신경망이 우리 생활에 커다란 영향을 줄 것이라는 근거

- 신경망을 훈련하기 위한 데이터가 많아짐
- 컴퓨터 하드웨어의 발전
- 훈련 알고리즘의 향상
- 일부 인공 신경망의 이론상 제한이 실전에서는 문제가 되지 않음
- 인공신경망이 투자와 진보의 선순환에 들어감

### 퍼셉트론

- 가장 간단한 인공 신경망 구조
- 입력과 출력이 이진값이 아닌 어떤 숫자이고, 각각의 입력 연결은 가중치와 연관됨
- **TLU**
    - 입력의 가중치 합 wT, x를 계산하고 편향 b를 더한 다음 계단 함수를 적용하는 인공 뉴런
        
        ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled.png)
        
    - 하나의 층 안에 놓인 하나 이상의 TLU로 구성되며, 각각의 TLU는 모든 입력에 연결됨 : **완전 연결 층**
- 퍼셉트론 학습 규칙은 오차가 감소되도록 연결을 강화시킴
- 잘못된 예측을 하는 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 입력에 연결된 가중치를 강화시킴
- 출력 뉴런의 결정 경계는 선형이므로 퍼셉트론은 복잡한 패턴 학습 불가능
    - 하지만, 훈련 샘플이 선형적으로 구분될 수 있다면 알고리즘이 정답에 수렴한다! ⇒ **퍼셉트론 수렴 이론**

### 다층 퍼셉트론과 역전파

- **입력층** 하나와 **은닉층**이라 불리는 하나 이상의 TLU 층과 마지막 출력 층으로 구성됨
    - 은닉 층을 여러 개 쌓아 올린 인공 신경망 : 심층 신경망(DNN)
- 그레이디언트를 자동으로 계산하고 경사 하강법 단계를 수행하는 과정을 반복하면 신경망의 오차가 점차 감소하여 결국 최솟값에 도달하게 됨
    - 후진 모드 자동 미분 + 경사 하강법 ⇒ **역전파**
- 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리하는 과정 반복 ⇒ **에포크**
- 역전파 알고리즘 요약
    - 먼저 미니배치에 대한 예측 만듦**(정방향 계산)**
    - 오차를 측정
    - 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도 측정**(역방향 계산)**
    - 이 오차가 감소하도록 가중치와 편향 조정**(경사 하강법)**
- 널리 쓰이는 다른 활성화 함수
    - tanh함수 : -1~1
    - ReLU함수 : <0이면 0
    - 왜 활성화 함수가 필요할까!!
        - 선형 변환을 여러 개 연결해도 얻을 수 있는 건 선형 변환뿐
        - 비선형성을 추가하지 않으면 아무리 층을 많이 쌓아도 하나의 층과 동일함

### 회귀를 위한 다층 퍼셉트론

- 사이키런 : MLPRegressor 클래스
    - 출력 층에서 활성화 함수를 지원하지 않음
    - 사이키런에서 신경망의 기능은 제한적임 ⇒ 케라스를 사용하는 이유!

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled%201.png)

### 분류를 위한 다층 퍼셉트론

- 출력 층에는 소프트맥스 활성화 함수를 사용해야 한다
- 확률 분포를 예측해야 하므로 손실 함수 ⇒ 크로스 엔트로피 손실
- 사이키런 : MLPClassifier 클래스

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled%202.png)

### 은닉 층 개수

- 복잡한 문제에서는 심층 신경망이 얕은 신경망보다 파라미터 효율성이 훨씬 좋음

### 은닉 층 뉴런 개수

- 모든 은닉 층에 같은 크기를 사용해도 동일하거나 더 나은 성능을 냄
- 데이터셋에 따라 다르지만 다른 은닉 층보다 첫번째 은닉 층을 크게 하는 것이 도움이 됨

### 학습률, 배치 크기 그리고 다른 하이퍼파라미터

- 학습률
    - 좋은 학습률을 찾는 방법 : 매우 낮은 학습률에서 시작해서 점진적으로 매우 큰 학습률까지 반복하여 모델을 훈련하는 것
- 옵티마이저
    - 평범한 미니배치 경사 하강법보다 더 좋은 옵티마이저를 선택하는 것도 매우 중요
- 배치 크기
    - 큰 배치 크기를 사용하는 장점 : 하드웨어 가속기를 효율적으로 사용 가능
    - 훈련 알고리즘이 초당 더 많은 샘플 처리 가능
    - 큰 배치 크기를 사용하면 훈련 초기에 불안정하게 훈련될수도!
- 활성화 함수
    - 출력 층의 활성화 함수는 수행하는 작업에 따라 달라짐
- 반복 횟수
    - 대부분의 경우 훈련 반복 횟수는 튜닝할 필요가 없으며, 대신 **조기 종료** 사용
    

## **11장.** 심층 신경망 훈련

---

### 그레이디언트 소실과 폭주 문제

- **그레이디언트 소실** : 경사 하강법이 하위 층의 연결 가중치를 변경되지 않은 채로 두어 훈련이 좋은 솔루션으로 수렴되지 않는것
- **그레디언트 폭주** : 그레이디언트가 점점 커져서 여러 층이 비정상적으로 큰 가중치로 갱신되어 알고리즘이 발산하는 문제
- 로지스틱 시그모이드 활성화 함수와 가중치 초기화 방식(평균이 0이고 표준편차가 1인 정규분포)의 조합일 때, 각 층에서 출력의 분산이 입력의 분산보다 더 큼

### 글로럿과 He 초기화

- ‘적절한 신호가 흐르기 위해서는 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 한다!’
- ‘적절한 신호가 흐르기 위해서는 역방향에서 층을 통과하기 전과 후의 그레이디언트 분산이 동일해야 함’

- 글로럿 초기화
- 르쿤 초기화
- He 초기화

### 고급 활성화 함수

- 활성화 함수를 잘못 선택하면 그레이디언트의 손실이나 폭주로 이어질 수 있음
- ReLU함수 ⇒ 특정 양숫값에 수렴하지 않고 계산도 빠름
    - 모든 샘플에 대해 ReLU 함수의 입력이 음수가 되면 뉴런은 죽게 됨
    - LeakyReLU 함수 등장
        - 하이퍼 파라미터 알파가 z<0일 때 함수의 기울기를 결정
        - z<0일 때 기울기가 LeakyReLU를 죽지 않게 만들어줌
        
        ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled%203.png)
        
        - 대규모 이미지 데이터셋에서는 ReLU보다 성능이 크게 앞서지만, 소규모 데이터셋에서는 훈련 세트에 과적합될 위험이 있음
            - 왜 과적합될 위험?
    - ReLU, LeakyReLU, PReLU ⇒ 매끄러운 함수가 아니라는 단점
        - 도함수가 z = 0에서 바뀌게 됨

### ReLU 활성화 함수 변형 활성화 함수

- **ELU 활성화 함수**
    
    ![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled%204.png)
    
    - z < 0일 때 음수값이 들어오므로 활성화 함수의 평균 출력이 0에 더 가까워짐
        - ⇒ 그레이디언트 소실 문제를 완화
        - 하이퍼파라미터 알파는 z가 큰 음숫값일 때 ELU가 수렴할 값의 역수를 정의
    - z < 0이어도 그레이디언트가 0이 아니므로 죽은 뉴런 만들지 않음
    - 알파 = 1이면 함수는 z = 0에서 급격히 변동하지 않으므로 z = 0을 포함해 모든 구간에서 매끄러워 경사 하강법의 속도를 높여줌
    - 지수함수를 사용하므로 ReLU나 그 변형들보다 계산이 느림
        - 속도가 빨라서 느린 계산이 상쇄될 수 있지만, 테스트 시에는 ReLU를 사용한 네트워크보다 더 느릴 것

- **SELU 활성화 함수**
    - ELU 활성화 함수의 변형
    - 훈련하는 동안 각 층의 출력이 평균 0과 표준편차 1을 유지하는 경향 있음
        - 그레이디언트 소실과 폭주 문제 막아줌

- **GELU**
- **SiLU**
- **Mish**

### 배치 정규화

- 배치 정규화 기법 : 각 층에서 활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가
    - 하나는 스케일 조정에, 다른 하나는 이동에 사용
    - 입력을 정규화한 다음, 스케일을 조정하고 이동시킴

### 그레이디언트 클리핑

- 역전파될 때 특정 임곗값을 넘어서지 못하게 그레이디언트를 잘라내는 것
- 배치 정규화를 사용하기 까다로운 순환 신경망에서 사용됨

### 사전 훈련된 층 재사용하기

- 전이학습 : 최상위 층을 제외하고 대부분의 층을 재사용
    - 훈련 속도를 크게 높일 뿐만 아니라 필요한 훈련 데이터도 그게 줄여줌

### 비지도 사전 훈련(?)

### 고속 옵티마이저

- **모멘텀 최적화**
- **네스테로프 가속 경사**
- **AdaGrad**
- **RMSProp**
- **Adam**
- **AdaMax**
- **Nadam**
- **AdamW**

- **학습률 스케줄링**
    - 학습률을 크게 잡으면 훈련이 발산, 너무 작게 잡으면 최적점에 수렴하겠지만 시간이 매우 오래 걸림
    - 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추면 최적의 고정 학습률보다 좋은 솔루션을 더 빨리 발견할 수 있음
        - 학습률을 감소시키는 전략 ⇒ **학습 스케줄**
            - **거듭제곱 기반 스케줄링**
            - **지수 기반 스케줄링**
            - **구간별 고정 스케줄링**
            - **성능 기반 스케줄링**
            - **1사이클 스케줄링**

### 규제를 사용해 과대적합 피하기

- **드롭아웃**
    - 드롭아웃으로 훈련된 뉴런은 이웃한 뉴런에 맞추어 적응할 수 없다
        - ⇒ 가능한 한 자기 자신이 유용해져야 함
        - ⇒ 몇 개의 입력 뉴런에만 지나치게 의존할 수 없음
        - ⇒ 모든 입력 뉴런에 주의를 기울여야 함
        - ⇒ 입력값의 작은 변화에 덜 민감해짐
    - **몬테 가를로 드롭아웃**
- **맥스-노름 규제**

![Untitled](2%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20Summary%2012cdb1cc159d45fabbd2bc949b1dd754/Untitled%205.png)