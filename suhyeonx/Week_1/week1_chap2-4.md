
# 2장 머신러닝 프로젝트 처음부터 끝까지
## 2.1 문제 정의
1) "비즈니스의 목적은?"
2) "현재 솔루션은 어떻게 구성되어 있나요?"
3) "어떤 지도 방식이 필요한가?"
ex) 지도 학습 - 다중 회귀 - 단변량 - 일반적인 배치 학습
- 데이터가 매우 크면 ([[맵리듀스MapReduce]] 기술 사용) - 배치 학습을 여러 서버로 분할, 또는 온라인 학습 기법 사용.

## 2.2 성능 측정 지표 선택
회귀 문제의 전형적인 성능 지표
- 평균 제곱근 오차 RMSE
- 평균 절대 오차 MAE - 이상치로 보이는 구역이 많을 때
- 노름 norm
	- 유클리드 노름 - 거리 계산. 제곱항을 합한 것의 제곱근
	- 맨해튼 노름 - 절대값의 합 계산
	
## 2.3 가정 검사 

## 2.4 데이터 이해를 위한 탐색과 시각화
1) 지리적 데이터 시각화하기
	ex) 산점도
2) 상관관계 조사하기
	 - 표준 상관계수 (pearson's r) - corr() 메서드 사
3) 특성 조합으로 실험하기
	ex) 구역 내 전체 방 개수 -> 가구당 방 개수 

## 2.5 머신러닝 알고리즘을 위한 데이터 준비
1) 데이터 정제
2) 텍스트와 범주형 특성 다루기
	- 원-핫 인코딩 (한 특성만 1이고 나머지는 0)
	- 특성을 다루기 쉽게 바꾸는 방법: 표현 학습 representation learning ex) - 임베딩embedding - 학습 가능한 저차원 벡터로 바꾸는 것.
3) 특성 스케일과 변화
	- min-max 스케일링 (정규화 normalization) - 0과 1사이로 제한
	- 표준화 - 특정 범위로 제한 x. 이상치에 영향 low
		ex) 특성 분포의 꼬리가 두꺼울 때 
			1. 특성을 제곱근으로 바꾼다. 
			2. 버킷타이징bucketizing
			- 역변환 주의
4) 사용자 정의 변환기
5) 변환 파이프라인

## 2.6 모델 선택과 훈련
1) 훈련 세트에서 훈련하고 평가하기
	- 결정 트리
2) 교차 검증으로 평가하기
	1. k-폴드 교차 검증
		- 훈련 세트를 폴드fold라 부르는 중복되지 않은 10개의 서브셋으로 랜덤 분할
		- 모델을 10번 훈련하고 평가하는데 매번 다른 폴드 사용
	- 랜덤 포레스트
		- 특성을 랜덤으로 선택해서 많은 결정 트리를 만들고 예측의 평균을 구함. 
		- 앙상블ensemble: 서로 다른 모델들로 구성된 모델

## 2.7 모델 미세 튜닝
1) 그리드 서치: 만족할 만한 하이퍼파라미터 조합을 찾을 때까지 수동으로 하이퍼파라미터를 조정
- GridSearchCV
2) 랜덤 서치 - RandomizedSearchCV - 하이퍼파라미터 탐색 공간이 커질 때
3) 앙상블 방법 -> 7장 
4) 최상의 모델과 오차 분석

## 2.8 론칭, 모니터링, 시스템 유지 보수
- 론칭할 때 다시 읽어보자..

## 2.9 직접 해보자.






# 3장 머신 러닝 분류 시스템

# 3.1 MNIST
잘 알려진 데이터셋. OpenML.org 에서 다운 가능.

# 3.2 이진 분류기 훈련
확률적 경사 하강법Stochastic Gradient Descent (SGD)

# 3.3 성능 측정
1. 교차 검증을 사용한 정확도 측정
- 정확도는 성능 측정 지표로 적당하지 않음.
	- 특히 불균형 데이터셋을 다룰 때
2. 오차 행렬confusion matrix
![[Pasted image 20240105002051.png]]
3. 정밀도precision와 재현율recall
	1. F<sub>1</sub> 점수: 정밀도와 재현율의 조화 평균
	- 점수가 높아지려면 재현율과 정밀도가 모두 높아야한다.
4. 정밀도/재현율 트레이드오프
 - 적당한 결정 임곗값 정하기
 - 임곗값이 높을수록 재현율은 낮아지고 반대로 (보통) 정밀도는 높아진다.
 - 누군가가 "99% 정밀도를 달성하자" 라고 말하면 반드시 "재현율 얼마에서?" 라고 물어봐야 한다.
5. ROC 곡선
수신기 조작 특성reciever operating characteristic(ROC)
- 거짓 양성 비율에 대한 진짜 양성 비율의 곡선
- 민감도(재현율)에 대한 1-특이도 그래프
- roc_curve() 함수
- 좋은 분류기는 완전한 랜덤 분류기의 roc 곡선에서 멀리 떨어저야함. 왼쪽 위 모서리.
- 곡선 아래의 면적area under the curve(AUC)로 비교


# 3.4 다중 분류
- 여러 전략 소개
# 3.5 오류 분석
- 성능 향상 방안에 대한 인사이트
- 훈련 데이터를 더 모으거나
- 특성을 찾아보거나
- 이미지 전처리
- **데이터 증식data augmentation** -> 7장
# 3.6 다중 레이블 분류
여러 개의 이진 꼬리표를 출력하는 분류 시스템
- ClassifierChain 클래스
	- 각 모델에 적절한 레이블 공급
# 3.7 다중 출력 분류
다중 레이블 분류에서 한 레이블이 다중 클래스가 될 수 있도록 일반화한 것.
(값을 2개 이상 가질 수 있음)
- 이미지에서 잡음을 제거하는 시스템
- 픽셀의 강도를 담은 배열 출력
- 분류기의 출력이 다중 레이블이고 각 레이블이 여러 개를 가짐.







# 4장 머신 러닝 모델 훈련
# 4.1 선형 회귀
- 닫힌 형태의 방정식 - 파라미터 직접 계산
- 경사 하강법 - 파라미터 수렴해가기
선형 모델: 입력 특성의 <span style="background:rgba(240, 200, 0, 0.2)">가중치 합</span>과 <span style="background:rgba(240, 200, 0, 0.2)">편향bias</span>이라는 상수를 더해 예측을 만든다.
목표: RMSE를 최소화하는 파라미터 찾기

==정규 방정식normal equation==
:해석적인 방법. 결과를 바로 얻을 수 있는 수학 공식
- 유사역행렬, 특잇값 분해(SVD)
- 역행렬 계산
- 계산 복잡도 - O(n<sup>2</sup>)

# 4.2 경사 하강법(GD)gradient descent
여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘
파라미터 벡터에 대해 비용 함수의 현재 그레이디언트gradient를 계산한다.
- 세타를 임의의 값으로 시작(랜덤 초기화)
- 조금씩 비용 함수가 감소되는 방향으로 진행
- 중요한 파라미터 - 스텝의 크기. 학습률 하이퍼파라미터로 결정.
- 전역 최솟값, 지역 최솟값
-> '모델의 파라미터 공간에서 찾는다'

- 비용함수의 편도함수
- **에포크**epoch: 훈련 세트를 한 번 반복하는 것
- 적절한 학습률을 찾는 것이 중요.

#### 1. 배치 경사 하강법
- 매 스텝에서 전체 훈련 세트를 사용한다.
#### 2. 확률적 경사 하강법
- 매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 계산한다.
#### 3. 미니배치 경사 하강법
- 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산
- SGD보다 최솟값에 더 가까이 도달하게 될 것이지만
	- 지역 최솟값에서 빠져나오기는 더 힘들 수.

![[Pasted image 20240105005619.png]]
# 4.3 다항 회귀
- PolynomialFeatures
- 규제 기법

# 4.4 학습 곡선
- 고차 다항 회귀 모델에서 과대적합되는 경우 vs 과소적합
1. 교차 검증을 통해 확인
2. 학습 곡선 확인
	- 모델의 훈련 오차와 검증 오차를 훈련 반복 횟소의 함수로 나타낸 그래프
	- 훈련 세트와 검증 세트에서 일정한 간격으로 모델을 평가하고 그 결과를 그래프로 그린다.
	- learning_curve()
	- 훈련 오차, 검증 오차 검토
- 편향/분산 트레이드 오프


# 4.5 규제가 있는 선형 모델
과대적합 줄이기
#### 모델 규제 
- 자유도, 다항식 차수를 줄이기
- 모델의 가중치를 제한

### 릿지 회귀
규제항이 추가된 선형 회귀 버전.
- 훈련하는 동안에만 비용 함수에 추가
- 하이퍼파라미터 a가 모델을 얼마나 규제할지 조절

### 라쏘 회귀
규제항을 추가한다. ㅣ<sub>2</sub> 노름 대신 가중치 벡터의 ㅣ<sub>1</sub>노름 사용
- 덜 중요한 특성의 가중치를 제거하려고 한다.
- 자동으로 특성 선택을 수행하고 희소 모델을 만든다.
- 비용 함수가 세타 = 0에서 미분 불가능
	- 서브그레이디언트 벡터 g 사용

### 엘라스틱넷
릿지와 라쏘를 절충한 모델
혼합 정도는 혼합 비율 r을 사용.
- 릿지가 기본이 되지만 몇 가지 특성만 유용하다 -> 라쏘, 엘라스틱넷
- 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 라쏘가 문제 발생 -> 엘라스틱넷

### 조기 종료
검증 오차가 최솟값에 도달하면 바로 훈련 중지

### 로지스틱 회귀
- 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용
- 선형 회귀처럼 바로 결과를 출력하지 않고 결괏값의 로지스틱을 출력한다.
	- 로지스틱 = 0과 1사이의 값을 출력하는 시그모이드 함수
	- 양성 샘플, 음성 샘플

### 소프트맥스 회귀 (다항 로지스틱 회귀)
로지스틱 회귀 모델은 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화될 수 있다.
- 파라미터 벡터들은 파라미터 행렬에 행으로 저장
- 크로스 엔트로피 비용 함수



