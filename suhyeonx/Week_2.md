# 10장 케라스를 사용한 인공 신경망 소개
인공 신경망 artificial neural network(ANN):
뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델

# 10.1 생물학적 뉴런에서 인공 뉴런까지
- 1943년 매컬러&피츠 최초의 인공신경망 구조 논문
- 생물학적 뉴런 -> 인공 뉴런
	- 뉴런을 이용한 인공 연산
### 퍼셉트론
가장 간단한 인공 신경망 구조
![[Pasted image 20231229011010.png]]- 계단 함수 사용
	- 헤비사이드 계단 함수 heaviside step function
- 훈련 알고리즘 - 헤브의 규칙에서 영감
	- 헤브의 규칙 (헤브 학습 hebbian learning): 두 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가하는 경향이 있다. 

사이킷런 - perceptron class

### 다층 퍼셉트론(MLP)
- 입력 층 + 은닉 층 + 출력 층
- 입력 층과 가까운 층 - 하위 층, 반대는 상위 층
모든 그레디언트를 자동으로 효율적으로 계산하는 알고리즘
**후진 모드 자동 미분**reverse-mode automatic differentiation (reverse-mode autodiff)
: 네트워크를 두 번 통과(전진, 후진)하면 모든 단일 모델 파라미터에 대한 신경망 오차의 그레이디언트를 계산할 수 있다. 

### 역전파backpropagation (backprop)
후진 모드 자동 미분 + 경사 하강법

### 비선형함수
계단 함수를 로지스틱 함수(시그모이드 함수)로 바꿈.
->수평선밖에 없으니 계산할 그레이디언트가 없음.
1) tanh 함수 
2) ReLU 함수

### 다층 퍼셉트론 적용
- 회귀
- 분류

# 10.2 케라스로 다층 퍼셉트론 구현하기

# 10.3 신경망 하이퍼파라미터 튜닝하기

하이퍼 파라미터의 종류
- 은닉 층 개수
- 은닉 층의 뉴런 개
	일반적으로 층의 뉴런 수보다 층 수를 늘리는 쪽이 이득이 더 많다. 
- 학습률
- 옵티마이저
- 배치 크기




# 11장 심층신경망 훈련
문제1:
==그레이디언트 소실 & 폭주==
안정화 필요

해결1:
### 초기화initialization
각 층의 연결 가중치를 랜덤으로 초기화
	- 글로럿 초기화 (세이비어 초기화)
	- 르쿤 초기화LeCun
	- He 초기화 (카이밍 초기화)


해결2:
ReLU는 완벽 x. - 죽은 ReLU 문제
### 고급활성화 함수 사용
- LeakyReLU
- ELU
- SELU
- GELU
	- SiLU(Swish)

해결3:
### 배치 정규화batch normalization
활성화 함수를 통과하기 전이나 후에 모델에 연산 추가
1. 입력을 원점에 맞추고 정규화
2. 각 층에서 두 개의 새로운 파라미터로 결과값의 스케일을 조정, 이동
	 하나는 스케일 조정, 하나는 이동

장점:
- 그레이디언트 소실 문제가 크게 감소 -> 수렴성을 가진 활성화 함수 사용 가능 
	ex) tanh, 시그모이드
 - 가중치 초기화에 네트워크가 훨씬 덜 민감
 - 큰 학습률
 - 규제와 같은 역할

단점:
- 모델의 복잡도를 키운다.
- 에포크마다 시간은 더 걸리지만 수렴이 훨씬 빨라져 보통 상쇄된다. 


해결4:
### 그레이디언트 클리핑
역전파될 때 특정 임곗값을 넘어서지 못하게 그레이디언트를 잘라내기.
- 일반적으로 배치 정규화를 사용하기 까다로운 순환 신경망에서 사용된다. 



# 11.2 사전 훈련된 층 재사용하기
전이 학습: 비슷한 유형의 문제를 처리한 신경망을 재사용하기. 
-> 훈련 속도 up, 필요한 훈련 데이터도 줄어든다.
- 비지도 사전 훈련
- 보조 작업에서 사전 훈련


# 11.3 고속 옵티마이저
- 모멘텀 최적화
- 네스테로프 가속 경사
	- AdaGrad
		- 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빨리 감소. -> 적응적 학습률
		  -> 전역 최적점 방향으로 더 곧장 가도록 갱신됨.
	- Adam
		- 적응적 모멘트 추정. 모멘텀 최적화 + RMSProp
	- AdaMax, Nadam, AdamW..

- 학습률 스케줄링
	- 거듭제곱, 지수, 구간별 고정, 성능,..

# 11.4 규제를 사용해 과대적합 피하기
1. l<sub>1</sub>과  l<sub>2</sub>규제
2. 드롭아웃
	-  매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃될 확률 p를 가짐.
	- 하이퍼파라미터 p = 드롭아웃 비율
	- 각 훈련 스텝에서 고유한 네트워크가 생성
	- 과대적합 되면 드롭아웃 비율 높게
	- 수렴을 느리게 만드는 경향이 있지만 적절한 튜닝은 좋음.
	- 몬테 카를로 드롭아웃
	
3. 맥스-노름 규제

# 11.5 요약 및 실용적인 가이드라인
