# 첫 대회 첫 시도 회고

요약: 엉망진창 

총 실험 회수는 약 20회.
f1 score 0.9 달성을 목표로 하기보단 케라스를 다뤄보는 데에 초점이 있었다. 
(진행 중에 pretained model(trainable)의 T/F를 껐다 켜보거나, 배치 정규화를 넣었다 빼보거나)
이번 주를 발판 삼아 다음에 다시 이 대회를 시작부터 천천히 해보는 거로 하자. 
그 때도 실험은 적당히 해보기. 

이번 케라스 사용기의 고칠점:
1. 기록 하나씩 꼼꼼히 확인하기
	기록을 하려고 했음에도 제대로 안된 부분이 있었다.
	ex) 단순 기록 실수, 실험에서 누락된 코드, 수정한 부분 인지 미스,, 등등..
	 그러다 보니까 꼬여서 다시 베이스라인으로 돌아가기도 했다.
2. 변화는 조금씩, 한 변수마다.
	한꺼번에 여러 변화를 주려고 한 적도 있었다.
	한 번에 다 추가해서 확 점수를 올리려고 한 것 같다. 
3. 복습 먼저
4. 다른 사람들의 시도 정황 보기. 
	왜 그런 시도가 있었는지 파악하기. 
	너무 맨 땅에 해딩하듯이 시도한 것도 화근. 
5. 집착 노노.. 스트레스 받지 말기. 훈련 시간 적응하기.

**변하지 않은 것:**
model: vgg16 
size: 331 

**시도해 본 것들:**
에포크 -> 30
batch.normalization layer 추가
adam->nadam
pretained_model_train = true
학습률 조정.

**알게 된 것:**
- 전이 학습은 사전 훈련된 모델에서 레이어를 가져오는 것이다. 
	 `layer.trainable`을 `False`로 설정하면 모든 레이어의 가중치가 훈련 가능에서 훈련 불가능으로 이동합니다. 이를 레이어 "동결"이라고 한다.
	 반대로 `True`로 설정하면 가중치가 훈련 가능하다. 
- early stopping - patience 충분히 늘리지 않으면 너무 빨리 종결됨.
- model에 마음대로 layer 추가하니까 모델이 망가졌다.

ex)
![image](https://github.com/GDSC-Hongik/2023-2-Hands-On-Machine-Learning-Study/assets/80656423/2be1c0d1-11db-4c34-b945-728582e6dbdf)

![image](https://github.com/GDSC-Hongik/2023-2-Hands-On-Machine-Learning-Study/assets/80656423/ebd91d0f-d93b-4527-89dc-85706d8cea6b)

- 개념들을 헷갈렸던 게 너무 많아서 복습 한 번하고 봐야겠다. 
